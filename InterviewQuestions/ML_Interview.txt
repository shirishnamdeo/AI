https://sebastianraschka.com/faq/
https://docs.google.com/document/d/10XqvDZsXXqlpcgXS3kzkllNsZaPD11B_tr0mhN1DOcQ/edit#heading=h.9o5sd4yz3zkz
https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-dimensionality-reduction-pca/
https://alekhyo.medium.com/interview-questions-on-pca-9cdc96ddaa9f
https://github.com/youngjeong46/hands-on-machine-learning/blob/master/TrainingModelQuestions.md
https://github.com/youngjeong46/hands-on-machine-learning/blob/master/MachineLearningQuestions.md

https://github.com/youngjeong46/hands-on-machine-learning/blob/master/classification.ipynb
https://github.com/youngjeong46/hands-on-machine-learning/blob/master/Housing.ipynb

https://stats.stackexchange.com/questions/359015/ridge-lasso-standardization-of-dummy-indicators
https://stats.stackexchange.com/questions/399430/does-categorical-variable-need-normalization-standardization


https://www.youtube.com/@AppliedAICourse

----------------------------------------------------------------------------------------------------

Miscelleneous-IQ
MIQ-001. Parametric Learning Algorithm vs Non Parametric Learning Algorithms?
MIQ-002. Convex vs Non-Convex Cost function? What does it mean when a cost function is non-convex?
MIQ-003. Machine-Learning vs Deep-Learning?
MIQ-004. Machine-Learning vs Deep-Learning -> What to choose for a project?
M1Q-005. Unreasonable Effectiveness of data?
M1Q-006. Active vs Passive Learning?
M1Q-007. Semi-Supervised Learning (SSL)?
M1Q-008. Un-Supervised Learning? Examples?
M1Q-009. Out-Of-Bag (OOB) Error/Estimates?
M1Q-010. Out-Of-Bag vs Cross-Validation?
M1Q-011. Scenarios where Decision-Tree preferred over Random-Forest?
MIQ-012. Online vs Offline Machine Learning? Exmaples?
MIQ-013. No-Free-Lunch Theorem?
MIQ-014. Incremental Learning? List of supported algorithms?
MIQ-015. Out-Of-Core Machine Learning?
MIQ-016. Structured vs Unstructured Data?
MIQ-017. Bagging (Bootstrap Aggregating) vs Boosting?
MIQ-018. How to measure the accuracy of a Clustering Algorithm?
MIQ-019. Lazy and Eager Learning?
MIQ-020. What is Matrix Factorization in ML? Where it it used?
MIQ-021. What is ALS Alternate Least Square?
MIQ-022. Matrix factorisation using side information?
MIQ-023. How to deal with Imbalance Dataset?
MIQ-024. Ways to make model more robust to outliers?
MIQ-025. What is Data Leakage? How to reduce it?
MIQ-026. Normalization vs Standardization?
MIQ-027. Evaluate preformance of dimensionality reduction algorithm?
MIQ-028. How to detect Multicolinearity?
MIQ-029. How to avoid Multicolinearity?
MIQ-030. Ways to reduce Overfitting?
MIQ-031. Types of bias in Machine Learning?
MIQ-032. Categorical feature with high cardinality?
MIQ-033. What is ROC-AUC curve? List some of it’s benefits.
MIQ-034. Why does L1 regularization give sparse coefficients?
MIQ-035. Ways to improve a model’s performance?
MIQ-036. What cross-validation technique would you use on a time series data set?
MIQ-037. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation?
MIQ-038. When Your Dataset Is Suffering From High Variance, How Would You Handle It?
MIQ-039. Overcome high Bias?
MIQ-040. Model Accuracy vs Model Performance? Which is important?
MIQ-041. Causation vs Correlation?
MIQ-042. Standard-Scaler vs Minmax-Scaler?
MIQ-043. What is OvR and OvO for multiclass classification and which machine learning algorithm supports this?
MIQ-044. When will be OVO method better than OVR method in multi-class SVM?
MIQ-045. What is the difference between loss function and cost function? Objective function?
MIQ-046. What are the common ways to handle missing data in a dataset?
MIQ-047. How to perform standardization for categorical variable?
MIQ-048. Which type of sampling is better for a classification model and why?
MIQ-049. Difference between Covariance and Correlation?
MIQ-050. List all types of popular recommendation systems?
MIQ-051. What types of model tend to overfit?
MIQ-052. What are some advantages and disadvantages of regression models?
MIQ-053. What are some advantages and disadvantages of tree based models?
MIQ-054. Describe complete life cycle of a data science project?
MIQ-055. What are the properties of a good ML model?
MIQ-056. Different evaluation metrices for a regression model?
MIQ-057. Different evaluation metrices for a classificaion model?
MIQ-058. Why does one hot encoding improve machine learning performance?
MIQ-059. What is Dummy variable trap?
MIQ-060. Which metrics can be used to measure correlation of categorical data?
MIQ-061. Why binary_crossentropy and categorical_crossentropy give different performances for the same problem?
MIQ-062. Why sometimes it is needed to scale or normalise features?
MIQ-063. Difference between Multilabel and MultiClass classification?
MIQ-064. What is KL divergence, how would you define its usecase in ML?

MachineLearning-IQ
MLIQ-001. [Naive_Bayes] What is Naive in Naive-Bayes?
MLIQ-003. [KNN] Why KNN is known as a Lazy-Learner?
MLIQ-004. [Losigtic_Regression] Why Logistic Regression is called Regression?
MLIQ-005. [GBM] Explain Gredient-Boosting?
MLIQ-006. [AdaBoost] Explain Adaptive-Boosting?
MLIQ-007. [XGBoost] Explain Extreme-Gredient-Boosting?
MLIQ-008. [LightGBM] Explain LightGBM?
MLIQ-009. [CatBoost] Explain CatBoost?
MLIQ-010. [Linear_Regression] Assumption of Linear-Regressions?
MLIQ-011. [Recommendation-Engine] How to measure accuracy of a Recommendation-Engine?
MLIQ-012. [Decision_Tree] Explain Pruning?

MLIQ-014. [Decision_Tree] Gini Impurity vs Entropy? Which one is better and why?
MLIQ-015. [PCA] Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?
MLIQ-016. [Linear_Regression] Importance of intercept term in a regression model?
MLIQ-017. [Cross_Validation] What is cross validation and its types?
MLIQ-018. [Ridge_Regression] Why is Ridge Regression called Ridge?
MLIQ-019. [PCA] Chaining two different dimensionality reduction algorithms?
MLIQ-020. [Decision_Tree] If a Decision Tree is underfitting the training set, does feature scaling helps?

MLIQ-031. [Linear_Regression] How do we interpret weights in linear regression?
MLIQ-032. [Losigtic_Regression] How do you interpret weights in logistic regression?
MLIQ-033. [Optimizer] Which Gradient Descent Algorithm will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?
MLIQ-034. [Lasso_Regression] How will you do feature selection using Lasso Regression?
MLIQ-035. [XGBoost] What are some important hyperparameters for XGBOOST?
MLIQ-036. [OPtimizer] Difference between Stochastic-Gradient-Descent(SGD) & Gradient-Descent(Batch)?
MLIQ-037. [K_Means] How to select K for K-means?


[Solutions-At:MLNotes->SVM]
SVM-001. [SVM] What are kernels in SVM? List some popular SVM kernels?
SVM-002. [SVM] What is the fundamental idea behind Support Vector Machines?
SVM-003. [SVM] What is a support vector?
SVM-004. [SVM] Why is it important to scale the inputs when using SVMs?
SVM-005. [SVM] Can an SVM classifier output a confidence score during classification? or Probability?
SVM-006. [SVM] Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?
SVM-007. [SVM] Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease γ (gamma)? What about C?
SVM-008. [SVM] How to train SVM on multi-class classification problem?
SVM-009. [SVM] Explain Kernel trick in SVM?
SVM-010. [SVM] What is the role of the 'Gamma' parameter in the RBF kernel, and how does it affect the SVM model?
SVM-011. [SVM] How can you prevent overfitting when using the Kernel Trick in SVMs?
SVM-011. [SVM] Does SVM face issues with large datasets? What/Why are those issues?
SVM-012. [SVM] Combining SVM and SGD?
SVM-013. [SVM] LinearSVC is much faster than SVC(kernel='linear'), especially if the training set is very large or if it has plenty of features?
SVM-014. [SVM] How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?
SVM-015. [SVM] Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model?
SVM-016. [SVM] Hard Margin vs Soft Margin?


DeepLearning-IQ


Statistics-IQ
SIQ-001: Example of when False-Positive is more crucial than False-Negative and vice versa?
SIQ-002: Example where Median is a better measure than the Mean?
SIQ-003. Probability vs Likelihood?
SIQ-004. What is P-value and its important?
SIQ-005. Difference between Type 1 and Type 2 error?

Scenario-Based-IQ
SBIQ-001. Imagine you are woking with a laptop of 2GB RAM, how would you process a dataset of 10GB?

----------------------------------------------------------------------------------------------------

67. Difference between R2 and adjusted R2? Why do you preffer adjusted r2?

69. What do you mean by Curse of Dimensionality?

70. What do you mean by Bias variance tradeoff?

72. What is the main difference between Machine Learning and Data Mining?

75. What is the difference between a Generative model vs a Discriminative model?

78. Considering the long list of machine learning algorithm, given a data set, how do you decide which one to use?

79. Differentiate between wide and tall data formats?

80. What is the difference between inductive machine learning and deductive machine learning?

81. How will you know which machine learning algorithm to choose for your classification problem?

83. How will you find the correlation between a categorical variable and a continuous variable ?

84. What are the differences between “Bayesian” and “Frequentist” approach for Machine Learning?

86. What is the difference between Gaussian Mixture Model and K-Means Algorithm?

87. Is more data always better?

88. How can you determine which features are the most im- portant in your model?

89. Which hyper-parameter tuning strategies (in general) do you know?

91. Describe the differences between and use cases for box plots and histograms

94. Can you define the concept of Undersampling and Oversampling?

95. Considering a Long List of Machine Learning Algorithms, given a Data Set, How Do You Decide Which One to Use?

97. List the most popular distribution curves along with scenarios where you will use them in an algorithm.

----------------------------------------------------------------------------------------------------
MIQ-001. Parametric Learning Algorithm vs Non Parametric Learning Algorithms?

[RESOURCE] https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html

Non-Parametric does not mean that they have NO parameters.
On the contrary, non-parametric models (can) become more and more complex with an increasing amount 
of data.

Example of Parametric Algorithms:
    Linear Regression
    Logistic Regression
    Linear Support Vector Machines


Example of Non-Parametric Algorithms:
    K-nearest neighbor
    Decision trees
    RBF kernel SVMs


So, in a parametric model, we have a finite number of parameters, and in nonparametric models, the 
number of parameters is (potentially) infinite. Or in other words, in nonparametric models, the 
complexity of the model grows with the number of training data; in parametric models, we have a 
fixed number of parameters (or a fixed structure if you will).

Linear models such as linear regression, logistic regression, and linear Support Vector Machines are
typical examples of a parametric “learners;” here, we have a fixed size of parameters (the weight 
coefficient.) In contrast, K-nearest neighbor, decision trees, or RBF kernel SVMs are considered as 
non-parametric learning algorithms since the number of parameters grows with the size of the 
training set. – K-nearest neighbor and decision trees, that makes sense, but why is an RBF kernel 
SVM non-parametric whereas a linear SVM is parametric? In the RBF kernel SVM, we construct the 
kernel matrix by computing the pair-wise distances between the training points, which makes it 
non-parametric.

Q. How KNN is Non-Parametric Algorithm?

----------------------------------------------------------------------------------------------------
MIQ-003. Machine-Learning vs Deep-Learning?

----------------------------------------------------------------------------------------------------
MIQ-004. Machine-Learning vs Deep-Learning -> What to choose for a project?

DeepLearning-Favour:
    Preformance better than Machine Learning
    Need to model Complex Problems (Complex relationships), and does not have domain knowledge to 
    create rules/features. Example: Images, Sounds.


DeepLearning-Against:
    Requires Lagre dataset (large labeled data is costly)
    Expensive (Hardware Cost, Taining Cost, and Training time).
    Explainability/Interpretability

----------------------------------------------------------------------------------------------------
M1Q-005. Unreasonable Effectiveness of data?

The phrase'unreasonable effectiveness of data' in machine learning refers to the remarkable ability 
of large datasets to produce highly accurate models, even when the algorithms used to analyze them 
are relatively simple.

The reason that large datasets are so effective at driving feature learning is that they contain a 
vast amount of information about the underlying patterns in the data.

----------------------------------------------------------------------------------------------------
M1Q-006. Active vs Passive Learning?

https://www.geeksforgeeks.org/passive-and-active-learning-in-machine-learning/

----------------------------------------------------------------------------------------------------
M1Q-007. Semi-Supervised Learning (SSL)?

https://www.ibm.com/topics/semi-supervised-learning

----------------------------------------------------------------------------------------------------
SIQ-001: Example of when False-Positive is more crucial than False-Negative and vice versa?

False-Positive more crucial than False-Negative:
    Spam Classification, when a legit email (Class-1/Class-Ture) is classified as Spam


False-Nagative more crucial than False-Positive:
    Medical Domain/Fraud Detection where we misclass a patient to be healthy.

----------------------------------------------------------------------------------------------------
MLIQ-001. What is Naive in Naive-Bayes?

https://www.youtube.com/watch?v=m9UaxSQJQGQ&ab_channel=CampusX

Naive-Bayes assumes that all the features are independent to each other, which does not really hold 
in real world problems.

----------------------------------------------------------------------------------------------------
MIQ-019. Lazy and Eager Learning?

https://www.geeksforgeeks.org/what-is-the-difference-between-lazy-and-eager-learning/
https://ai.plainenglish.io/lazy-vs-eager-learning-the-tortoise-and-the-hare-of-machine-learning-27ffb14f9c08

Lazy learning, also known as instance-based learning or data-driven learning, defers the learning 
process until the time of prediction.

Lazy-Learning-Pros:
    Adaptability to Complex Patterns: Lazy learning excels when the decision boundaries are complex 
    and non-linear. New data points seamlessly integrate, requiring minimal retraining.

    Continuous Learning: The model can adapt to new data points without requiring a retraining phase

    Efficiency: They process relevant data for each prediction, saving resources.

    Flexibility: Lazy learning allows for more flexibility in terms of data preprocessing and 
    transformation, as it can be done on-the-fly during training.

    Scalability: Lazy learning can be more scalable to large datasets, as it only processes the data 
    that is needed for the current iteration of the training process.


Lazy-Learning-Cons:
    Computational Overhead: Prediction can be computationally expensive, especially with large 
    datasets. Higher memory usage, storing all data can strain resources for large datasets.



Example Lazy-Learner:
    KNN

Example Eager-Learner:
    Linear-Regression
    Decision-Trees

----------------------------------------------------------------------------------------------------
M1Q-011. Scenarios where Decision-Tree preferred over Random-Forest?

https://www.youtube.com/watch?v=1pIrDi6puGs&ab_channel=CampusX

Explainability
Less-Expesive Computation
Features Selection (When we want certain features to be in the model, as Random forest can drop them)

----------------------------------------------------------------------------------------------------
MLIQ-004. [Losigtic_Regression] Why Logistic Regression is called Regression?

It works just like a regression, with a sigmoid function applied over it which translate the 
real-valued output of regression function to [0-1] range probabilities, which can further be 
classifed into classes using a threshold.

----------------------------------------------------------------------------------------------------
MIQ-012. Online vs Offline Machine Learning? Exmaples?

https://www.qwak.com/post/online-vs-offline-machine-learning-whats-the-difference
https://medium.com/value-stream-design/online-machine-learning-515556ff72c5


Online machine learning means that learning takes place as data becomes available.
Online learning is ideal for machine learning systems that receive data as a continuous flow and 
need to be able to adapt to rapidly changing conditions. 
Also useful when resources are limited.

Only algorithms having partial_fit methods to incrementally train support online learning.

----------------------------------------------------------------------------------------------------
MIQ-013. No-Free-Lunch Theorem?

https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/

The theorem states that all optimization algorithms perform equally well when their performance is 
averaged across all possible problems.

----------------------------------------------------------------------------------------------------
SBIQ-001. Imagine you are woking with a laptop of 2GB RAM, how would you process a dataset of 10GB?

Online-ML Alrorithms
Sub-Sampling of dataset (to train only on a sub-set of data). Will lose important information.
Using Cloud Computing (if allowed)

Out-Of-Core ML

----------------------------------------------------------------------------------------------------
MIQ-014. Incremental Learning? List of supported algorithms?

https://www.datacamp.com/blog/what-is-incremental-learning

Needs data readers (for streaming data)



https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=pipelines-incremental-learning-details

Algorithms for classification models that support incremental learning
    ExtraTreesClassifier
    XGBClassifier
    LGBMClassifier
    RandomForestClassifier
    SnapRandomForestClassifier
    SnapBoostingMachineClassifier

Algorithms for regression models that support incremental learning
    ExtraTreesRegressor
    LGBMRegressor
    RandomForestRegressor
    SnapBoostingMachineRegressor
    SnapRandomForestRegressor
    XGBRegressor

----------------------------------------------------------------------------------------------------
MIQ-017. Bagging (Bootstrap Aggregating) vs Boosting?


Bagging
    Aim to decrease variance, not bias.
    Each model receives equal weight.
    Each model is built independently.
    Different training data subsets are selected using row sampling with replacement and random 
    sampling methods from the entire training dataset.
    Bagging tries to solve the over-fitting problem.
    Classifiers are trained parallelly.

Boosting
    Aim to decrease bias, not variance.
    Models are weighted according to their performance.
    New models are influenced by the performance of previously built models.
    Every new subset contains the elements that were misclassified by previous models.
    Boosting tries to reduce bias.
    Classifiers are trained sequentially.


----------------------------------------------------------------------------------------------------
MLIQ-010. [Linear_Regression] Assumption of Linear-Regressions?

1. Linear-Model
2. No-Multicolinearlity
3. Homoscedasticity of Residuals or Equal Variances
4. No Autocorrelation in residuals
5. Predictors are distributed Normall

6. Each observation is unique
5. Number of observations Greater than the number of predictorsy

----------------------------------------------------------------------------------------------------
MIQ-018. How to measure the accuracy of a Clustering Algorithm?

https://medium.com/@divine_inner_voice/cracking-the-code-of-clustering-accuracy-metrics-unveiled-fe4e13f6940a

Silhouette Score
Adjusted Rand Index (ARI)

----------------------------------------------------------------------------------------------------
MIQ-020. What is Matrix Factorization in ML? Where it it used?


----------------------------------------------------------------------------------------------------
MIQ-022. Matrix factorisation using side information?

https://www.youtube.com/watch?v=HdI5lmOKNTs&ab_channel=AppliedAICourse

----------------------------------------------------------------------------------------------------
MIQ-023. How to deal with Imbalance Dataset?

Over-Sampline
Under-Sampling
SMOTE (Synthetic Minority Oversampling Technique)

Threshold Moving

BalancedBaggingClassifier

----------------------------------------------------------------------------------------------------
MIQ-024. Ways to make model more robust to outliers?

https://www.kdnuggets.com/2018/08/make-machine-learning-models-robust-outliers.html

Winsorizing
This method involves setting the extreme values of an attribute to some specified value.

Log-Scale Transformation
Binning


----------------------------------------------------------------------------------------------------
MIQ-025. What is Data Leakage? How to reduce it?

https://builtin.com/machine-learning/data-leakage

In short, data leakage in machine learning is a term used to describe a case where the data used to 
train an algorithm includes unexpected additional information about the subject it’s evaluating.


----------------------------------------------------------------------------------------------------
MIQ-027. Evaluate preformance of dimensionality reduction algorithm?

https://www.linkedin.com/advice/0/how-can-you-evaluate-dimensionality-reduction-ygrwf

1. Data reconstruction error
2. Data compression ratio
3. Data visualization quality
4. Data classification accuracy[vs before the dimensionalityR, in case of supervised classification]


----------------------------------------------------------------------------------------------------
MIQ-028. How to detect Multicolinearity?

https://www.theanalysisfactor.com/eight-ways-to-detect-multicollinearity/

1. VIF
2. Examining Correlation Matrix
3. Eigenvalue decomposition of the correlation matrix.

----------------------------------------------------------------------------------------------------
MIQ-029. How to avoid Multicolinearity?


Alternatively, you can use a different regression technique such as ridge regression or principal 
component regression that is better equipped to handle multicollinearity than ordinary least squares 
regression. 

----------------------------------------------------------------------------------------------------
MIQ-030. Ways to reduce Overfitting?

https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d


1. Early Stopping
2. Pruning 
3. Feature Selection
4. Regularization
5. Ensembling
6. Cross-Validation
7. Data Augmentation


----------------------------------------------------------------------------------------------------
MIQ-031. Types of bias in Machine Learning?

https://www.kdnuggets.com/2019/08/types-bias-machine-learning.html

1. Sample Bias
2. Prejudice Bias
3. Confirmation Bias
4. Group attribution Bias

https://www.taus.net/resources/blog/9-types-of-data-bias-in-machine-learning
Selection Bias
Measurement Bias (and more)

----------------------------------------------------------------------------------------------------
MIQ-032. Categorical feature with high cardinality?

https://stackoverflow.com/questions/61585507/how-to-encode-a-categorical-feature-with-high-cardinality
https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13
https://stats.stackexchange.com/questions/411767/encoding-of-categorical-variables-with-high-cardinality
https://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159

Target Encoding
Count Encoding
Feature hashing
Embedding



https://www.linkedin.com/advice/0/how-do-you-deal-categorical-features-high-cardinality
Grouping by Frequency
Grouping by Similarity
Grouping by Target

----------------------------------------------------------------------------------------------------
MLIQ-012. [Decision_Tree] Explain Pruning?

Minimum Leaf Size


Types:
Pre-Pruning (Early Stopping)
    Maximum Depth
    Minimum Samples per leaf
    Minimum Samples per split
    Maximum Features

Post-Pruning (Reducing Nodes)
    Cost-Complexity Pruning (CCP)
    Reduced Error Pruning
    Minimum Impurity Decrease
    Minimum Leaf Size

----------------------------------------------------------------------------------------------------
MIQ-033. What is ROC-AUC curve? List some of it’s benefits.

https://www.youtube.com/watch?v=4jRBRDbJemM&ab_channel=StatQuestwithJoshStarmer


ROC stands for Receiver Operating Characteristics, and the ROC curve is the graphical representation 
of the effectiveness of the binary classification model. 
It plots the true positive rate (TPR) vs the false positive rate (FPR) at different classification 
thresholds.

----------------------------------------------------------------------------------------------------
MLIQ-014. [Decision_Tree] Gini Impurity vs Entropy? Which one is better and why?

https://quantdare.com/decision-trees-gini-vs-entropy/

Gini Index has values inside the interval [0, 0.5] whereas the interval of the Entropy is [0, 1]. 

On the one hand, the gini criterion is much faster because it is less computationally expensive.

On the other hand, the obtained results using the entropy criterion are slightly better.

Nevertheless, as the results are so similar, it does not seem to be worth the time invested in 
training when using the entropy criterion.

----------------------------------------------------------------------------------------------------
MIQ-034. Why does L1 regularization give sparse coefficients?

https://satishkumarmoparthi.medium.com/why-l1-norm-creates-sparsity-compared-with-l2-norm-3c6fa9c607f4

----------------------------------------------------------------------------------------------------
MIQ-035. Ways to improve a model’s performance?

1. Add More Data. Use Large high quality dataset.
2. Treat Missing and Outlier Values
3. Feature Engineering
4. Feature Selection
5. Multiple Algorithms
6. Algorithm Tuning
7. Ensemble Methods
8. Cross Validation
9. Hyperparameter tuning
10.Regularization

----------------------------------------------------------------------------------------------------
MLIQ-015. [PCA] Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?

However, PCA is traditionally based on linear assumptions, which may not hold for nonlinear datasets
leading to inefficiencies. Recent research has explored nonlinear PCA methods that use neural 
network approaches to suit data with nonlinear structures


----------------------------------------------------------------------------------------------------
MIQ-036. What cross-validation technique would you use on a time series data set?

https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4



In the case of time series, the cross-validation is not trivial. We cannot choose random samples and
assign them to either the test set or the train set because it makes no sense to use the values from 
the future to forecast values in the past.

The method that can be used for cross-validating the time-series model is cross-validation on a 
rolling basis.
Start with a small subset of data for training purpose, forecast for the later data points and then 
checking the accuracy for the forecasted data points. The same forecasted data points are then 
included as part of the next training dataset and subsequent data points are forecasted.


----------------------------------------------------------------------------------------------------
MIQ-037. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation?

It's usually not possible to perfectly reverse the dimensionality reduction of a dataset because 
some information is lost during the process. However, it's possible to estimate the original 
dataset's appearance with a high degree of accuracy. 
For example, in the case of MNIST, it's possible to decompress a reduced dataset by applying the 
inverse transformation of the PCA projection. 


----------------------------------------------------------------------------------------------------
MLIQ-016. [Linear_Regression] Importance of intercept term in a regression model?

https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model

The intercept term, also known as the constant term, is usually included in regression models to 
ensure the model is unbiased and to calculate accurate predictions

Unbiased estimates
The intercept ensures the mean of the residuals is zero, which is crucial for unbiased estimates of 
the slope and accurate predicted values.


----------------------------------------------------------------------------------------------------
MIQ-038. When Your Dataset Is Suffering From High Variance, How Would You Handle It?

Cross-Validation
Ensembling

----------------------------------------------------------------------------------------------------
MIQ-040. Model Accuracy vs Model Performance? Which is important?

https://www.fiddler.ai/model-accuracy-vs-model-performance/which-is-more-important-model-performance-or-model-accuracy

Need to define what is Model Performance? Is it execution time? Then depends on the problem.
But if it means how well your ML solution is accomplishing the required task, then Model Performance
is much important.


Model performance is generally considered more important than model accuracy. While accuracy 
measures the percentage of correct predictions, performance is a broader assessment of how well a 
model accomplishes its intended task. Depending on the application, performance metrics may include 
accuracy, real-time performance, and adaptability

----------------------------------------------------------------------------------------------------
MLIQ-017. [Cross_Validation] What is cross validation and its types?

https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations

1. K-fold cross-validation
2. Hold-out cross-validation [train/test split]
3. Stratified k-fold cross-validation [Maintains class representation]
4. Leave-p-out cross-validation
5. Leave-one-out cross-validation
6. Monte Carlo (shuffle-split) [Different % of train/test splot]
7. Time series (rolling cross-validation)


----------------------------------------------------------------------------------------------------
MLIQ-018. [Ridge_Regression] Why is Ridge Regression called Ridge?

https://math.stackexchange.com/questions/695352/whats-the-ridge-in-ridge-regression

It is named after ridge analysis. ("ridge" refers to the path from the constrained maximum).

----------------------------------------------------------------------------------------------------
MIQ-041. Causation vs Correlation?

https://amplitude.com/blog/causation-correlation


While causation and correlation can exist simultaneously, Correlation does not imply Causation.

Causation means one thing causes another—in other words, action A causes outcome B. On the other 
hand, correlation is simply a relationship where action A relates to action B—but one event doesn’t 
necessarily cause the other event to happen.

----------------------------------------------------------------------------------------------------
MLIQ-019. [PCA] Chaining two different dimensionality reduction algorithms?

https://www.brainscape.com/flashcards/dimensionality-reduction-8761174/packs/14725107


Yes, it can make sense to chain two different dimensionality reduction algorithms. 
For example, a common practice is to use PCA to quickly remove many useless dimensions, and then 
apply a slower algorithm like LLE.

Benefits: Chaining algorithms can be useful when one algorithm is fast at removing useless 
dimensions, and another is better at preserving important information.

Drawbacks: Dimensionality reduction can cause information loss.

----------------------------------------------------------------------------------------------------
MLIQ-020. [Decision_Tree] If a Decision Tree is underfitting the training set, does feature scaling helps?

No, scaling the input features is generally not necessary to fix underfitting in a decision tree. 
Decision trees are not sensitive to feature scaling because their splits don't change with any 
monotonic transformation.

----------------------------------------------------------------------------------------------------
MLIQ-031. [Linear_Regression] How do we interpret weights in linear regression?

https://www.theanalysisfactor.com/interpreting-regression-coefficients/

Numerical-Features 
Categorical-Features

----------------------------------------------------------------------------------------------------
MLIQ-032. [Losigtic_Regression] How do you interpret weights in logistic regression?

https://www.quora.com/How-do-you-interpret-weights-in-logistic-regression

----------------------------------------------------------------------------------------------------
MLIQ-033. [Optimizer] Which Gradient Descent Algorithm will reach the vicinity of the optimal 
solution the fastest? Which will actually converge? How can you make the others converge as well?

https://github.com/youngjeong46/hands-on-machine-learning/blob/master/TrainingModelQuestions.md

The Stochastic Gradient Descent will reach the fastest since you are using one random training data 
at each iteration. However, the Batch Gradient Descent is the only one to actually converge. You 
cannot make the others converge; they will only approach close to the global minimum

----------------------------------------------------------------------------------------------------
SIQ-004. What is P-value and its important?

The P value is defined as the probability under the assumption of no effect or no difference 
(null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. 
The P stands for probability and measures how likely it is that any observed difference between 
groups is due to chance.

----------------------------------------------------------------------------------------------------
MIQ-043. What is OvR and OvO for multiclass classification and which machine learning algorithm 
supports this?

https://wadhwatanya1234.medium.com/multi-class-classification-one-vs-all-one-vs-one-993dd23ae7ca
https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/

OvO -> One-vs-One
OvR -> One-vs-Rest

----------------------------------------------------------------------------------------------------
MIQ-044. When will be OVO method better than OVR method in multi-class SVM?

https://typeset.io/questions/when-will-be-ovo-method-better-than-ovr-method-in-multi-25hidd86mq

----------------------------------------------------------------------------------------------------
MLIQ-034. [Lasso_Regression] How will you do feature selection using Lasso Regression?

https://developer.ibm.com/tutorials/awb-lasso-regression-automatic-feature-selection/

Lasso stands for Least Absolute Shrinkage and Selection Operator. It is frequently used in machine 
learning to handle high dimensional data as it facilitates automatic feature selection. It does this
by adding a penalty term to the residual sum of squares (RSS), which is then multiplied by the 
regularization parameter (lambda or λ). This regularization parameter controls the amount of 
regularization applied. Larger values of lambda increase the penalty, shrinking more of the 
coefficients towards zero, which subsequently reduces the importance of (or altogether eliminates) 
some of the features from the model, which results in automatic feature selection. Conversely, 
smaller values of lambda reduce the effect of the penalty, retaining more features within the model.


----------------------------------------------------------------------------------------------------
MIQ-045. What is the difference between loss function and cost function? Objective function?

https://nadeemm.medium.com/cost-function-loss-function-c3cab1ddffa4

Loss function
Measures the difference between the actual and predicted values for a single record. It's directly 
related to the predictions of the model.

Cost function
Measures the model's error on a group of objects. It's an average of the loss functions over the 
entire training set.

Objective function
While training a model, we minimize the cost (loss) over the training data. However, its low value 
isn’t the only thing we should care about. The generalization capability is even more important 
since the model that works well only for the training data is useless in practice.

So, to avoid overfitting, we add a regularization term that penalizes the model’s complexity. That 
way, we get a new function to minimize during training:

In general, the objective function is the one we optimize, i.e., whose value we want to either 
minimize or maximize. The cost function, that is, the loss over a whole set of data, is not 
necessarily the one we’ll minimize, although it can be. For instance, we can fit a model without 
regularization, in which case the objective function is the cost function.

----------------------------------------------------------------------------------------------------
MIQ-046. What are the common ways to handle missing data in a dataset?

Delete rows or columns
This straightforward approach can reduce sample size, introduce bias, and lose information. 
It's also known as case-wise deletion or list-wise deletion.

Impute missing values
This involves replacing missing data with a value. You can use a single value, like the mean, 
median, or mode, or multiple values, like a distribution. However, using averages or midpoints can 
introduce bias.

Use regression methods
These methods can predict missing values and are often used to fill in missing data.

Use advanced techniques
For example, you can use K-Nearest Neighbors (KNN) to estimate missing values by finding similar 
data points

----------------------------------------------------------------------------------------------------
MIQ-048. Which type of sampling is better for a classification model and why?

There isn't a single best sampling method for classification models, as the method's effectiveness 
depends on the learning algorithm, training dataset, and its density and composition. However, 
stratified sampling is often used to ensure representative sampling of classes in a dataset, 
especially imbalanced datasets

Simple random sampling: One of the best probability sampling techniques that helps in saving time 
and resources is the Simple Random Sampling method. It is a reliable method of obtaining 
information where every single member of a population is chosen randomly, merely by chance.

----------------------------------------------------------------------------------------------------
MIQ-042. Standard-Scaler vs Minmax-Scaler?

MinMaxScaler scales the data to a fixed range, typically between 0 and 1. On the other hand, 
StandardScaler rescales the data to have a mean of 0 and a standard deviation of 1. This results in 
a distribution with zero mean and unit variance

----------------------------------------------------------------------------------------------------
MLIQ-035. [XGBoost] What are some important hyperparameters for XGBOOST?

Commonly adjusted parameters include learning rate (eta), maximum tree depth (max_depth), and 
minimum child weight (min_child_weight).

----------------------------------------------------------------------------------------------------
SIQ-005. Difference between Type 1 and Type 2 error?

In statistical hypothesis testing, a type I error is when a null hypothesis is rejected when it's 
actually true, while a type II error is when a null hypothesis is not rejected when it's actually 
false.

Type I errors are also known as false positives, while type II errors are also known as false 
negatives.

----------------------------------------------------------------------------------------------------
MIQ-049. Difference between Covariance and Correlation?

https://www.mygreatlearning.com/blog/covariance-vs-correlation/

Covariance indicates the direction of the linear relationship between variables. 
Correlation on the other hand measures both the strength and direction of the linear relationship 
between two variables.

Covariance can vary between -∞ and +∞
Correlation ranges between -1 and +1

Covariance is affected by the change in scale. If all the values of one variable are multiplied by 
a constant and all the values of another variable are multiplied by a similar or different constant,
then the covariance is changed.
Correlation is not influenced by the change in scale.

Covariance formula vs Correlation formula?

----------------------------------------------------------------------------------------------------
MIQ-050. List all types of popular recommendation systems?

1. Collaborative Filtering
2. Content-Based Filtering
3. Hybrid Recommendation Systems


----------------------------------------------------------------------------------------------------
MIQ-051. What types of model tend to overfit?

Nonparametric and nonlinear models12are more likely to overfit. These models have more flexibility 
when learning a target function, which can lead to unrealistic models. 

Many nonparametric machine learning algorithms include parameters or techniques to limit and 
constrain how much detail the model learns1

For example, decision trees are a nonparametric machine learning algorithm that is very flexible 
and is subject to overfitting training data. This problem can be addressed by pruning a tree after 
it has learned in order to remove some of the detail it has picked up.

----------------------------------------------------------------------------------------------------
MLIQ-036. [OPtimizer] Difference between Stochastic-Gradient-Descent(SGD) & Gradient-Descent(Batch)?

The key difference compared to standard (Batch) Gradient Descent is that only one piece of data from
the dataset is used to calculate the step, and the piece of data is picked randomly at each step.
In order to avoid bad local minima, SGD uses a noisy estimate of the gradient - a random gradient, 
whose expected value is the true gradient4

Advantages and Disadvantages of Stochastic Gradient Descent?
Advantages and Disadvantages of Gradient Descent?

https://paperswithcode.com/method/sgd-with-momentum

SGD vs Mini-Batch GD?

----------------------------------------------------------------------------------------------------
MIQ-052..What are some advantages and disadvantages of regression models?

Advantages:
Interpretability: Linear regression models are straightforward to interpret. The coefficients 
represent the impact of each feature on the target variable.

No Feature Scaling Required: Unlike some other algorithms, linear regression is not sensitive to 
feature scaling (e.g., normalization or standardization). **Is this true?

Dis-advantages:
Sensitive to Outliers: Outliers can significantly affect the model’s performance.

Limited Complexity: Linear regression cannot capture complex patterns in the data, non-linear 
relaionships.

----------------------------------------------------------------------------------------------------
MIQ-053..What are some advantages and disadvantages of tree based models?

Advantages:
Interpretability: Decision trees are transparent and easy to understand.

Non-linear Relationships: Can capture complex, non-linear relationships.

No Feature Scaling Required: Insensitive to feature scaling.

Handles Both Numerical and Categorical Data: Convenient for mixed data types.


Dis-advantages:
Prone to overfitting, especially when they grow too deep or when the dataset is noisy.

High variance, meaning small changes in the training data can result in significantly different 
trees.

Instability: Small changes in data can lead to different tree structures. Trees can also be 
sensitive to small changes in the data, as they may create different splits and structures depending
on the order and randomness of the data.

Bias Toward Dominant Classes: In classification, decision trees tend to favor dominant classes.

----------------------------------------------------------------------------------------------------
MIQ-054. Describe complete life cycle of a data science project?

Business understanding: Defining the objectives, requirements, and constraints of the project.
Data collection: Gathering data from relevant sources.
Data preparation: Cleaning, processing, and structuring the data.
Exploratory data analysis: Identifying patterns, biases, and ranges in the data.
Model building: Creating a model to achieve the desired performance.
Model evaluation: Assessing the effectiveness of the model.
Model deployment: Integrating the model into existing systems or processes.
Monitoring: Continuously monitoring the performance of the model and making adjustments as needed.

----------------------------------------------------------------------------------------------------
MIQ-055. What are the properties of a good ML model?

Generalization
The model should make accurate predictions on unseen data. It needs to learn patterns from the 
training data that can be applied effectively to new, previously unseen examples1.

Simplicity
A good model is simple and interpretable. Simplicity makes it easier to understand, explain, and 
debug. Complex models might perform well on the training data but can be harder to interpret and 
may overfit.

Robustness
The model should handle noise and outliers in the data gracefully. Robustness ensures that the 
model’s performance remains stable even when faced with unexpected variations or errors.


----------------------------------------------------------------------------------------------------
MIQ-056. Different evaluation metrices for a regression model?

MAE
MSE
RMSE
R-Squared
MAPE

----------------------------------------------------------------------------------------------------
MIQ-057. Different evaluation metrices for a classificaion model?

Accuracy
Confusion-Matrix
Precission
Recall
F1-Score
AUC-ROC-Curve

----------------------------------------------------------------------------------------------------
MIQ-058. Why does one hot encoding improve machine learning performance?

Avoid/Remove Ordinality issues

However, one-hot encoding can also introduce multicollinearity and the dummy variable trap, which 
can affect some regression models.

----------------------------------------------------------------------------------------------------
MIQ-059. What is Dummy variable trap?

The dummy variable trap occurs when we create the same number of dummy variables as the number of 
unique values a categorical variable can take on. Let me explain further. 

When using categorical variables (like marital status or eye color) as predictors in regression 
models, we create dummy variables. These variables take on values of either 0 or 1, representing 
the presence or absence of a specific category. For instance, if we have three marital status 
categories (“Single,” “Married,” and “Divorced”), we create two dummy variables (k-1) to avoid 
redundancy. Let’s say “Single” is our baseline category. The dummy variables would be:

Married: 1 if married, 0 otherwise
Divorced: 1 if divorced, 0 otherwise
However, the trap occurs when we mistakenly create k dummy variables (in this case, three). This 
leads to multicollinearity, where at least two dummy variables are perfectly correlated. 

For example, if we had:

Single: 1 if single, 0 otherwise
Married: 1 if married, 0 otherwise
Divorced: 1 if divorced, 0 otherwise
Here, “Single” and “Married” are perfectly correlated (with a correlation coefficient of -1). 
This causes incorrect calculations of regression coefficients and p-values. To avoid the trap, 
always create k-1 dummy variables! 


----------------------------------------------------------------------------------------------------
MLIQ-037. [K_Means] How to select K for K-means?

Elbow Method
    Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of (k).
    Choose the (k) for which WSS starts to diminish significantly.

Silhouette Method
    The silhouette value measures how similar a point is to its own cluster (cohesion) compared to 
    other clusters (separation).
    Silhouette values range from +1 to -1, with higher values being desirable.
    Calculate silhouette scores for different (k) values and choose the one with the highest average
    silhouette score.

----------------------------------------------------------------------------------------------------
MIQ-060. Which metrics can be used to measure correlation of categorical data?

Tetrachoric Correlation: This metric is used to calculate the correlation between binary categorical
variables. Binary variables have only two possible values. The tetrachoric correlation ranges from 
-1 (strong negative correlation) to 1 (strong positive correlation). For example, if you want to 
assess whether gender is associated with political party preference, you can use tetrachoric 
correlation.

Polychoric Correlation: Polychoric correlation is employed for ordinal categorical variables. 
Ordinal variables have a natural order (e.g., ratings on a scale). The polychoric correlation also 
ranges from -1 to 1. For instance, if you’re comparing movie ratings from two different agencies 
(rated on a scale of 1 to 3), you can use polychoric correlation.

Cramer’s V: This metric is used to calculate the correlation between nominal categorical variables. 
Nominal variables have no inherent order. Cramer’s V ranges from 0 (no correlation) to 1 (strong 
correlation).

Analysis of variance (ANOVA) - Statistical-Test

----------------------------------------------------------------------------------------------------
MIQ-061. Why binary_crossentropy and categorical_crossentropy give different performances for the 
same problem?

https://stackoverflow.com/questions/42081257/why-binary-crossentropy-and-categorical-crossentropy-give-different-performances
https://www.youtube.com/watch?v=gwtrk6Ml1n8&ab_channel=LukeChaffey

----------------------------------------------------------------------------------------------------
MIQ-062. Why sometimes it is needed to scale or normalise features?

Gradient Descent Convergence: When training models using gradient-based optimization algorithms 
(like gradient descent), features with different scales can lead to slow convergence or even 
divergence. Scaling ensures that the optimization process is more stable and efficient.

Equal Influence: Features with larger scales can dominate the learning process, making other 
features less influential. Scaling ensures that all features contribute equally to the model.

Distance Metrics: Many algorithms (e.g., k-means clustering, k-nearest neighbors) rely on distance 
metrics. If features have different scales, the distance calculations can be biased. Scaling helps 
maintain consistent distances.

Regularization: Regularization techniques (e.g., L1, L2 regularization) penalize large coefficients. 
Scaling prevents features with large scales from dominating the regularization term. 

Model Interpretability: Scaled features make it easier to interpret model coefficients. 
For example, in linear regression, the coefficients represent the change in the target variable per 
unit change in the feature.

----------------------------------------------------------------------------------------------------
MIQ-063. Difference between Multilabel and MultiClass classification?

Multiclass classification assigns one label to each observation, while multilabel classification 
allows for multiple labels to be assigned to each observation.

----------------------------------------------------------------------------------------------------
MIQ-064. What is KL divergence, how would you define its usecase in ML?

KL Divergence helps us understand how much our model’s predictions deviate from the target. It can 
also be used post production during model monitoring to see how different the predictors and targets 
are different from the base distributions.


KL divergence, also known as Kullback-Leibler divergence, is a metric used to compare two 
probability distributions. It quantifies how much one distribution differs from another. 
Specifically, given two probability distributions P and Q, KL divergence measures the number of bits
required to convert P into Q.
[Copilot-Search]

----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------